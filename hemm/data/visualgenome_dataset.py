# -*- coding: utf-8 -*-
"""VisualGenome_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
		https://colab.research.google.com/drive/1pAPiREufnJssq8GkOG6DDsGU-tnJefK0
"""

import os
import json
from typing import Optional, Union, List
from PIL import Image
import torch
from tqdm import tqdm
import pandas as pd
import requests
from hemm.data.dataset import HEMMDatasetEvaluator
from hemm.utils.common_utils import shell_command
from hemm.prompts.visualgenome_prompt import visualgenomeprompt
from hemm.metrics.bertscore_metric import BertScoreMetric
from hemm.metrics.bleu_metric import BleuMetric

class VisualGenomeEvaluator(HEMMDatasetEvaluator):
	def __init__(self,
				questions_json_path="/work/agoindan/question_answers.json",
				device="cpu",
				):
		super().__init__()
		self.device = device
		self.prompt = visualgenomeprompt()
		self.questions_json_path = questions_json_path
		self.metrics = [BertScoreMetric(), BleuMetric()]

	def get_prompt(self, text):
		prompt_text = self.prompt.format_prompt(text)
		return prompt_text
	
	def __len__(self):
		f = open(self.questions_json_path)
		data_vqa = json.load(f)
		return len(data_vqa)

	def evaluate_dataset(self,
						model,
						) -> None:
		self.load()
		self.model = model

		predictions = []
		ground_truth = []

		f = open(self.questions_json_path)
		data_vqa = json.load(f)
		for i in tqdm(range(len(data_vqa)), total=len(data_vqa)):
			temp_dict=data_vqa[i]
			img_id=temp_dict['id']
			qas=temp_dict['qas']
			try:
				if i==1:
					url=f"https://cs.stanford.edu/people/rak248/VG_100K_2/{img_id}.jpg"
					image=Image.open(requests.get(url, stream=True).raw)
					image_b = image.resize((640,480))
				else:
					url=f"https://cs.stanford.edu/people/rak248/VG_100K/{img_id}.jpg"
					image=Image.open(requests.get(url, stream=True).raw)
					image_b = image.resize((640,480))
			except:
				continue

			for j in range(len(qas)):
				question=qas[j]['question']
				question_pmt=self.get_prompt(question)
				output = self.model.generate(question_pmt, image_b)
				predictions.append(output)
				ground_truth.append(qas[j]['answer'])

		return predictions, ground_truth

	def load(self):
		pass

	def evaluate_dataset_batched(self, model, batch_size=32):
		self.load()
		self.model = model

		images = []
		texts = []
		predictions = []
		ground_truth = []

		f = open(self.questions_json_path)

		data_vqa = json.load(f)
		for i in tqdm(range(len(data_vqa)), total = len(data_vqa)):
			temp_dict=data_vqa[i]
			img_id=temp_dict['id']
			qas=temp_dict['qas']
			try:
				if i==1:
					url=f"https://cs.stanford.edu/people/rak248/VG_100K_2/{img_id}.jpg"
					image=Image.open(requests.get(url, stream=True).raw)
					image_b = image.resize((640,480))
				else:
					url=f"https://cs.stanford.edu/people/rak248/VG_100K/{img_id}.jpg"
					image=Image.open(requests.get(url, stream=True).raw)
					image_b = image.resize((640,480))
			except:
				continue
			
			for j in range(len(qas)):
				question=qas[j]['question']
				question_pmt=self.get_prompt(question)
				texts.append(question_pmt)
				images.append(self.model.get_image_tensor(image_b))
				# raw_images.append(image_b)
				ground_truth.append(qas[j]['answer'])

		samples = len(images)
		predictions = self.predict_batched(images[:samples], texts[:samples], batch_size)
		# samples = len(raw_images)
		# self.save_details(raw_images[:samples], texts[:samples], ground_truth[:samples], "visualgen.pkl")
	
		return predictions, ground_truth[:samples]
	