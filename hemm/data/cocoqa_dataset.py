# -*- coding: utf-8 -*-
"""cocoqa_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mbzTxUmX9GDKiPfjdWxs5swiR41Xr4dW
"""

import os
import json
from typing import Optional, Union, List
from PIL import Image
import torch
from tqdm import tqdm
import pandas as pd

from hemm.data.dataset import HEMMDatasetEvaluator
from hemm.utils.common_utils import shell_command
from hemm.prompts.cocoqa_prompt import cocoprompt

class cocoqaEvaluator(HEMMDatasetEvaluator):
    def __init__(self,
                 questions_file,
                 device="cpu",
                 ):
        super().__init__()
        self.device = device
        self.prompt = cocoprompt()
        self.questions_file = questions_file

        def get_prompt(self, text):
          prompt_text = self.prompt.format_prompt(text)
          return prompt_text

        def evaluate_dataset(self,
                         model,
                         metric
                         ) -> None:
        self.load()
        self.model = model
        self.metric = metric

        predictions = []
        ground_truth = []

        df = pd.read_csv(self.questions_file)
        df.drop(['Unnamed: 0'],axis=1,inplace=True)
        for i in range(len(df)):
          img_id=df['final_image_id'][i]
          url=f"http://images.cocodataset.org/train2017/{img_id}.jpg"
          image=Image.open(requests.get(url, stream=True).raw)
          question=df['question_count'][i]
          question_prompt=self.get_prompt(question)
          output = self.model.generate(question_prompt, image)
          predictions.append(output)
          ground_truth.append(df['answer_count'][i])


        results = self.metric.compute(ground_truth, predictions)
        return results