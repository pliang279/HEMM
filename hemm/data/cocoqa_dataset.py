# -*- coding: utf-8 -*-
"""cocoqa_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1mbzTxUmX9GDKiPfjdWxs5swiR41Xr4dW
"""

import os
import json
from typing import Optional, Union, List
from PIL import Image
import torch
from tqdm import tqdm
import pandas as pd

from hemm.data.dataset import HEMMDatasetEvaluator
from hemm.utils.common_utils import shell_command
from hemm.prompts.cocoqa_prompt import cocoprompt
from hemm.metrics.bertscore_metric import BertScoreMetric
from hemm.metrics.bleu_metric import BleuMetric

class cocoqaEvaluator(HEMMDatasetEvaluator):
	def __init__(self,
				 questions_file,
				 device="cpu",
				 ):
		super().__init__()
		self.device = device
		self.prompt = cocoprompt()
		self.questions_file = questions_file
		self.metrics = [BertScoreMetric(), BleuMetric()]

	def get_prompt(self, text):
		prompt_text = self.prompt.format_prompt(text)
		return prompt_text

	def evaluate_dataset(self,
						model,
						) -> None:
		self.load()
		self.model = model

		predictions = []
		ground_truth = []

		df = pd.read_csv(self.questions_file)
		df.drop(['Unnamed: 0'],axis=1,inplace=True)
		for i in range(len(df)):
			img_id=df['final_image_id'][i]
			url=f"http://images.cocodataset.org/train2017/{img_id}.jpg"
			image=Image.open(requests.get(url, stream=True).raw)
			question=df['question_count'][i]
			question_prompt=self.get_prompt(question)
			output = self.model.generate(question_prompt, image)
			predictions.append(output)
			ground_truth.append(df['answer_count'][i])

		results = {}
		for metric in self.metrics:
			results[metric.name] = metric.compute(ground_truth, predictions)
		
		return predictions, results
	
	def load(self):
		pass

	def evaluate_dataset_batched(self):
		pass
	